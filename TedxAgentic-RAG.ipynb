{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b2bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing global dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "from tqdm.autonotebook import tqdm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ee9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing langchain dependencies\n",
    "import langchain\n",
    "import langchain\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a524859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Option 2: HuggingFace Pipeline (Local inference)\\nfrom langchain_huggingface import HuggingFacePipeline\\n# Option 3: Ollama (Local LLM server)\\nfrom langchain_community.llms import Ollama\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FREE LLM ALTERNATIVES\n",
    "# Option 1: Groq (Free tier with rate limits)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\"\"\"\n",
    "# Option 2: HuggingFace Pipeline (Local inference)\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "# Option 3: Ollama (Local LLM server)\n",
    "from langchain_community.llms import Ollama\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb61d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE EMBEDDING ALTERNATIVES\n",
    "# Option 1: HuggingFace Sentence Transformers (Free)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Option 2: Ollama Embeddings (Free local)\n",
    "# from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc6bc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE VECTOR STORE ALTERNATIVES  \n",
    "# Option 1: ChromaDB (Completely free)\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "# Option 2: FAISS (Free, in-memory)\n",
    "#from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Option 3: Keep Pinecone (has free tier, but limited)\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c425e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE SEARCH ALTERNATIVES\n",
    "# Option 1: DuckDuckGo Search (Free, no API key needed) - generally worse\n",
    "# from langchain_community.tools import DuckDuckGoSearchResults \n",
    "# Option 2: Keep Tavily if you have free tier\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa30450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENTS\n",
    "from langchain.agents import AgentExecutor, Tool, AgentType\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a188cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environmental variables from a .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dca182f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the datasets\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./Datasets/tedx-transcripts.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    source_column=\"transcript\",\n",
    "    metadata_columns= [\"main_speaker\", \"name\", \"speaker_occupation\", \"title\", \"url\", \"description\"]\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60bd330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words: 9\n",
      "Number of Characters: 62\n",
      "List of Tokens: [4438, 1690, 84296, 87, 3137, 61412, 527, 389, 279, 61412, 1773, 8534, 30]\n",
      "Nr of Tokens: 13\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "#In a given String the number of Tokens are counted by\n",
    "\n",
    "def num_tokens(question, encoding_name):\n",
    "    \n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = encoding.encode(question)\n",
    "\n",
    "    return encoding, num_tokens\n",
    "\n",
    "\n",
    "question = \"How many TEDx talk transcripts are on the transcripts-dataset?\"\n",
    "\n",
    "encoding, num_tokens = num_tokens(question, \"cl100k_base\")\n",
    "\n",
    "print(f'Number of Words: {len(question.split())}')\n",
    "print(f'Number of Characters: {len(question)}')\n",
    "print(f'List of Tokens: {num_tokens}')\n",
    "print(f'Nr of Tokens: {len(num_tokens)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2910581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Question: How many TEDx talk transcripts are on the transcripts-dataset?\n"
     ]
    }
   ],
   "source": [
    "#decoding tokenizer\n",
    "def decode_tokens(tokens, encoding):\n",
    "    return encoding.decode(tokens)\n",
    "\n",
    "decoded_question = decode_tokens(num_tokens, encoding)\n",
    "print(f'Decoded Question: {decoded_question}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
